{
 "metadata": {
  "name": "",
  "signature": "sha256:06415757b48ae3838037820585e940be46dbc7e8f61443a29aa3c64d706ea31e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model Fitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preamble\n",
      "\n",
      "- *Sami Niemi (s.niemi@ucl.ac.uk)*\n",
      "- *Data Analysis and Image Processing with Python, UCL/MSSL, November 4-5 2014*\n",
      "- *Notebook available on [github](https://github.com/sniemi/DataAnalysisPythonCourse)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook has been adapted from various examples presented at http://jakevdp.github.io and a great document titled [Data analysis recipes: Fitting a model to data](http://arxiv.org/abs/1008.4686)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What is model fitting?\n",
      "\n",
      "After we have inspected our data the next we must do is to define a model that we **think** describes the data.\n",
      "The *model* is usually some functional form with free parameters that we can adjust to find \"*best*\" fit.\n",
      "\n",
      "**How would you define \"best\"?**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "np.random.seed(0)\n",
      "\n",
      "x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
      "              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\n",
      "y = np.array([33, 70, 34, 35, 37, 75, 37, 44, 48, 49,\n",
      "              53, 49, 50, 45, 56, 60, 61, 63, 44, 71])\n",
      "e = np.array([ 3.3, 3.3, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n",
      "               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])\n",
      "\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**what model would you fit to the data?**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Frequentist Approach to Fitting a Line"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we think that our data can be described by a linear model *M* defined by a slope $\\theta_1$ and an intercept $\\theta_2$:\n",
      "\n",
      "$$\n",
      "M(x;\\theta_1, \\theta_2) = \\theta_1 x + \\theta_2\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Uncertanties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In reality no model will perfectly fit the data (or you should be suspicious about the data). In general, our data should be distributed around the model. As long as there are no systematic residuals and each data point is within a \"*reasonable*\" distance from the model, this is ok.\n",
      "\n",
      "**How do you define \"reasonable\"?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is very common to assume Normal (Gaussian) distribution $\\mathcal{N}$ for uncertanties. In this case we can say that the observed datum $y_i$ comes from the model $M$ with uncertanty $\\sigma$:\n",
      "\n",
      "$$\n",
      "y_i \\sim \\mathcal{N}[M, \\sigma]\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How do we find the values of the model parameters that fit the data \"best\"?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to fit a model to data, we need to measure in a quantitative way how well a model with given parameters fits the data.\n",
      "We need a function, sometimes called the cost function, that defines the metrics of the goodness of the fit.\n",
      "\n",
      "If we specify the cost function and minimize it we find the best fit.\n",
      "\n",
      "What cost function should we specify?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two often used ones are\n",
      "\n",
      "Euclidean Distance / L1 Norm:\n",
      "\n",
      "$$\n",
      "S_{L1} = \\sum_i \\left|y_i - M(x_i;\\theta)\\right|\n",
      "$$\n",
      "\n",
      "Least Squares / L2 Norm:\n",
      "\n",
      "$$\n",
      "S_{L2} = \\sum_i [ y_i - M(x_i;\\theta) ]^{2}\n",
      "$$\n",
      "\n",
      "In theory we could use any cost function we want. However, justifying a random cost function might be tricky...\n",
      "\n",
      "Lets look at an example."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example: Fitting a Line"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'create some fake data with uncertanties'\n",
      "x = np.array([ 0,  3,  9, 14, 15, 19, 20, 21, 30, 35,\n",
      "              40, 41, 42, 43, 54, 56, 67, 69, 72, 88])\n",
      "y = np.array([33, 70, 34, 35, 37, 75, 37, 44, 48, 49,\n",
      "              53, 49, 50, 45, 56, 60, 61, 63, 44, 71])\n",
      "e = np.array([ 3.3, 3.3, 2.6, 3.4, 3.8, 3.8, 2.2, 2.1, 2.3, 3.8,\n",
      "               2.2, 2.8, 3.9, 3.1, 3.4, 2.6, 3.4, 3.7, 2.0, 3.5])\n",
      "xfit = np.linspace(-1, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Straight line fit with NumPy, what do you think is the cost function?'\n",
      "theta = np.polyfit(x, y, 1, w=1./e)\n",
      "print theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'or by explictly writing the loss function'\n",
      "def L1(theta, x=x, y=y, e=e):\n",
      "    dy = y - theta[0] - theta[1]*x\n",
      "    return np.sum(np.abs(dy / e))\n",
      "\n",
      "def L2(theta, x=x, y=y, e=e):\n",
      "    dy = y - theta[0] - theta[1]*x\n",
      "    return np.sum((dy / e)**2)\n",
      "\n",
      "thetaA = optimize.fmin(L1, [1, 1])\n",
      "thetaS = optimize.fmin(L2, [1, 1])\n",
      "print thetaS\n",
      "print thetaA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "plt.plot(xfit, theta[1] + theta[0] * xfit, 'b--', lw=3, label='fit N')\n",
      "plt.plot(xfit, thetaA[0] + thetaA[1] * xfit, 'r--', lw=2, label='fit L1')\n",
      "plt.plot(xfit, thetaS[0] + thetaS[1] * xfit, color='lightgray', lw=2, label='fit L2')\n",
      "plt.legend(numpoints=1, shadow=True, fancybox=True, loc='best')\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "L1 looks a lot better than L2, why?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'We could also use something else, like Huber loss...'\n",
      "print 'But how do we justify this and the value of c?'\n",
      "\n",
      "def huberLoss(t, c=2.5):\n",
      "    \"\"\"\n",
      "    The Huber loss defines a critical value at which the loss curve transitions from quadratic to linear.\n",
      "    \"\"\"\n",
      "    return ((abs(t) < c) * 0.5 * t ** 2\n",
      "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
      "\n",
      "def totalHuberLoss(theta, x=x, y=y, e=e, c=2.5):\n",
      "    return huberLoss((y - theta[0] - theta[1] * x) / e, c).sum()\n",
      "\n",
      "theta2 = optimize.fmin(totalHuberLoss, [0, 0], disp=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "plt.plot(xfit, theta[1] + theta[0] * xfit, 'b--', lw=3, label='fit N')\n",
      "plt.plot(xfit, thetaA[0] + thetaA[1] * xfit, 'r--', lw=2, label='fit L1')\n",
      "plt.plot(xfit, thetaS[0] + thetaS[1] * xfit, color='lightgray', lw=2, label='fit L2')\n",
      "plt.plot(xfit, theta2[0] + theta2[1] * xfit, 'g--', lw=2, label='fit H')\n",
      "plt.legend(numpoints=1, shadow=True, fancybox=True, loc='best')\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Great, we now got plenty of fits but which one should we choose for our paper?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Likelihood"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How can we justify a cost function?\n",
      "\n",
      "We can think about this in a probabilistic way. For each $x_i$, we observe $y_i$. Lets consider now only a single datum $x_i$, any datum, for example $x_1$. For this datum the observed value $y_1$ is not necessarily the same as the \"true\" $y_{true}$, because our observations are limited (e.g. instrumental effects, noise, whatnot). We can therefore think that have a probability distribution which describes how likely we are to observe a particular $y$ given the true $y_{true}$. While we cannot ever know the true value, it is still the one we are trying to recover (for example with our model)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now ask, what is the probability of observing $y_1$ given the true value $y_{true}$ $P(y_1 \\, |\\, y_{true})$. To answer this, we need to assume something about our measurement uncertainties (note that I avoid using the word \"error\", because errors should be corrected).\n",
      "\n",
      "The most common choice is to use Normal distribution $\\mathcal{N}$. In many cases this can be justified using e.g. Central limit theorem or maximum entropy arguments. However, if you are in the low number regime and couting e.g. photons, then you have to use Poisson distribution $\\mathcal{P}$.\n",
      "\n",
      "If we assume that our uncertanties follow Normal distribution, we can consider that $y_1$ is a random variable, which is distributed like\n",
      "\n",
      "$$\n",
      "y_1 \\sim \\mathcal{N}[y_{true}, \\sigma^2]\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "ydiff = np.linspace(-6, 6, 1000)\n",
      "plt.plot(ydiff, stats.norm.pdf(ydiff, 0, 1.5))\n",
      "plt.xlabel('$y_1\\, - \\, y_{true}$', size=20)\n",
      "plt.ylabel('$P(y_1 \\, |\\, y_{true})$', size=20)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the assumed Normal distribution for a single value $y_1$, we can compute the probability $P$ of observing this point given the true $y_{true}$ and the uncertainty $\\sigma$:\n",
      "\n",
      "$$\n",
      "P(y_1 \\, | \\, y_{true},\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[\\frac{-(y_1 - y_{true})^2}{2\\sigma^2}\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### From a Single Point to Many\n",
      "\n",
      "The above equation gives the probability of observing a single point given the true and the uncertainty, called the likelihood (probability of data given model). In practice, we are scientists after all, we have many observation points, not only one, so the obvious question is how can we derive a likelihood for all the data points? This is the same as asking what is the joint probability of our points $y_i$ given the true values $y_{i, true}$ and associate uncertanties $\\sigma_{i}$. Thus, the question is what is:\n",
      "\n",
      "$$\n",
      "P(y_i \\, | \\, y_{i, true}, \\sigma_i) = \\quad ?\n",
      "$$\n",
      "\n",
      "While this equation looks simple, it can be complicated to evaluate. However, if we **assume** that all $y_i$ are **independent** of each other we can write:\n",
      "\n",
      "$$\n",
      "P(y_i \\, | \\, y_{i, true}, \\sigma_i) = \\prod_{i=1}^N P(y_i~|~y_{i, true},\\sigma)\n",
      "$$\n",
      "\n",
      "This is to say that if the idenpendence assumption holds we can simply multiply the individual probabilities together.\n",
      "\n",
      "This product of probabilities is known as the **likelihood** $L$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In practise, because these probabilities can become very small, we often take the logratihm of the above equation. In that case, the product turns into a sum, and we have the *log-likelihood*, which is often denoted as follows:\n",
      "\n",
      "$$\n",
      "\\mathcal{L} = \\log P(y_i \\, | \\, y_{i, true}, \\sigma_i)  = \\sum_{i=1}^N \\log P(y_i~|~y_{i, true},\\sigma)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given this formalism, we can now state that the model that maximises this likelihood is the model which best fits the data or in other words the model which gives our observed data the highest probability is the most likely model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Maximising the Likelihood\n",
      "\n",
      "So far we have found out that the model $M$ that maximises the likelihood $L$ can be considered the best fit.\n",
      "So how can be maximise the likelihood?\n",
      "\n",
      "If we assume that our uncertanties can be described by Normal distribution $\\mathcal{N}$, we get the following log-likelihood:\n",
      "\n",
      "$$\n",
      "\\mathcal{L} = \\log L = - \\frac{N_{\\textrm{pix}}}{2}\\log(2\\pi) - N_{\\textrm{pix}} \\sum_{i, j}^{N_{\\textrm{pix}}} \\log(\\sigma_{i, j}) -  \\sum_{i, j}^{N_{\\textrm{pix}}} \\frac{\\left(D_{i, j} - M_{i, j}\\right)^{2}}{2\\sigma_{i, j}^{2}} \\quad .\n",
      "$$\n",
      "\n",
      "If we are trying to maximise the above function we can omit the first term because this is a constant. The second term is also basically constant (albeit be careful with biased and unbiased estimators, this may not be true always). So, with these assumptions in mind, we are actually maximising:\n",
      "\n",
      "$$\n",
      "\\sum_{i, j}^{N_{\\textrm{pix}}} \\frac{\\left(D_{i, j} - M_{i, j}\\right)^{2}}{2\\sigma_{i, j}^{2}}\n",
      "$$\n",
      "\n",
      "This equation should look familiar. Anyone?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What we have found is that, in the presence of Normally distributed uncertanties that are independent, maximising the likelihood is identical to minimising the sum of squares (note the minus sign in front of the likelihood). This is the justification for the least-squares fitting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'But wait, we saw that the least-squares did not lead to a great fit. What gives?'\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "plt.plot(xfit, thetaS[0] + thetaS[1] * xfit, 'r-', lw=2, label='fit Least Squares')\n",
      "plt.legend(numpoints=1, shadow=True, fancybox=True, loc='best')\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian Approach to Fitting a Line"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In data analysis we are concerned with the question how good a description of the data a given model is. The aim is then to evaluate the posterior probability of the model $M$ given the observed data $D$ in presence of additional ``information'' $I$. The Bayesian paradigm dictates that inference about the model should be based on the probability of it given the data $P(M\\, |\\, D)$. If our model includes a set of parameters, denoted with $\\bar{\\theta}$, whose values we want to estimate from our data $D$, we can write Bayes' theorem as follows \n",
      "\n",
      "$$\n",
      "P(M, \\bar{\\theta} \\, |\\, D, I) = \\frac{P(D\\, |\\, M, \\bar{\\theta}, I)P(M, \\bar{\\theta}\\, |\\, I)}{P(D\\, |\\, I)} \\quad .\n",
      "$$\n",
      "\n",
      "In this case we have included additional information $I$ to the analysis by including probability distributions for all model parameters $P(M, \\bar{\\theta} \\, | \\, I)$ as priors.\n",
      "\n",
      "What are these terms?\n",
      "\n",
      "#### *posterior*: $P(M, \\bar{\\theta}~|~D,I)$\n",
      "This is the part we are interested in; the posterior probability of the model $M$ and its parameters $\\bar{\\theta}$ given data $D$ and information $I$.\n",
      "\n",
      "#### *likelihood*: $P(D~|~M, \\bar{\\theta},I)$\n",
      "This is the likelihood of the data $D$ given our model $M$, parameters $\\bar{\\theta}$, and information $I$ at hand.\n",
      "\n",
      "#### *prior*: $P(M, \\bar{\\theta}~|~I)$\n",
      "This probability function contains all information we have about the model $M$.\n",
      "\n",
      "#### *evidence*: $P(D~|~I)$\n",
      "This is a normalisation term, which in is usually very difficult to calculate. On the bright side, we usually don't need this :-)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example: Fitting a Line via Sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, to fit a line Bayesian style, we need a model $M$, log-likelihood $P(M \\, | \\, D, I)$, and priors $P(M \\, | \\, I)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def model(theta, x):\n",
      "    return theta[0] * x + theta[1]\n",
      "\n",
      "def log_pr(theta):\n",
      "    return 0\n",
      "\n",
      "def log_likelihood(theta, x, y, e):\n",
      "    y_model = model(theta, x)\n",
      "    return -0.5 * np.sum((y - y_model) ** 2 / (2 * e) ** 2)\n",
      "\n",
      "def log_post(theta, x, y, e):\n",
      "    return log_pr(theta) + log_likelihood(theta, x, y, e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then also need a way to draw samples from the posterior $P(M \\, | \\, D, I)$. Once these samples are drawn, we can use the characteristics of our sample to solve our problem. In the following we use [emcee](http://dan.iel.fm/emcee/) package to do this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import emcee\n",
      "from multiprocessing import Pool\n",
      "\n",
      "print emcee.__version__\n",
      "\n",
      "cores = 4 #run on 4 cores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndim = 2        # number of parameters in the model\n",
      "nwalkers = 50   # number of MCMC walkers\n",
      "nburn = 500     # \"burn-in\" period to let chains stabilize\n",
      "nsteps = 2000   # number of MCMC steps to take\n",
      "\n",
      "starting_guesses = np.random.rand(nwalkers, ndim)\n",
      "\n",
      "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_post, args=[x, y, e])\n",
      "sampler.run_mcmc(starting_guesses, nsteps)\n",
      "\n",
      "sample = sampler.chain  # shape = (nwalkers, nsteps, ndim)\n",
      "sample = sampler.chain[:, nburn:, :].reshape(-1, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!pip install triangle_plot\n",
      "import triangle\n",
      "triangle.corner(sample, labels=['slope', 'intercept']);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the marginalised distributions for both the slope and intercept are more or less Normally distried. There is however a correlation between the two parameters.\n",
      "\n",
      "If we want to see the \"best-fit\" parameter values, we can choose the maximum of the posterior to find the *Maximum a Posteriori* fit. In case where the marginalised distributions are Gaussian these are close to the mean of the samples:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_MAP = sample.mean(0)\n",
      "print(theta_MAP)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Lets plot this mean fit:'\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "plt.plot(xfit, thetaS[0] + thetaS[1] * xfit, 'r-', lw=2, label='fit Least Squares')\n",
      "plt.plot(xfit, model(theta_MAP, xfit), 'b--', lw=2, label='Mean Posterior Fit')\n",
      "plt.legend(numpoints=1, shadow=True, fancybox=True, loc='best')\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What, we got the same result out? Why waste my time?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note though that in the Bayesian problem, the result is not a single number. The Bayesian result is the full posterior, which our lines have sampled. Lets plot 300 hundred of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "indices = np.random.choice(np.arange(len(sample)), 300)\n",
      "\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data')\n",
      "for i in indices:\n",
      "    plt.plot(xfit, model(sample[i], xfit), '-k', alpha=0.02)\n",
      "plt.xlim(-1, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Still rubbish, what gives?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The problem is that our model is wrong. In the beginning of the session I said that we define a model that we **think** describes the data.\n",
      "\n",
      "We assumed, incorrectly it seems, that all data were generated by a process that can be described by a line. What we should have realised is that there are at least three points that are highly suspicious. Perhaps we went on a coffee break and the measurements were rubbish.\n",
      "\n",
      "What we need is a model that is flexible enough to deal with measurements that may be outliers. We could just fudge these data points out, sigma-clip, \"forget\", or what not. Or we could just bite the bullet and do the Right Thing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Example: Fitting a Line with Outliers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets adopt a more complicated model that has the flexibility to account for outliers. One option is to choose a mixture between a signal and a nuisance background."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could assume that our data points come either from the real expirement or blunders. If we want to explicitly and objectively reject bad data points, we must add to the problem a set of N nuisance parameters $g_i$, one per data point. These nuisance parameters $g_i$ can be thought as weights which range from 0 to 1 and encode for each point $i$ the degree to which it fits the model, so that $g_i=0$ indicates an outlier, in which case a Gaussian of width $\\sigma_B$ is used in the computation of the likelihood. This $\\sigma_B$ can also be a nuisance parameter, or alternatively its value can be set at a sufficiently high number (say 50). All these extra parameters may seem unnecessary, but their values can be inferred and marginalized out so in the end, we will be left with an inference just of the line parameters.\n",
      "\n",
      "We can now write the likehood $P(D~|\\, \\bar{\\theta},I)$ as a sum of the signal and the background:\n",
      "\n",
      "$$\n",
      "\\begin{array}{ll}\n",
      "P(x_i\\, y_i, e_i~|~\\bar{\\theta},g_i,\\sigma,\\sigma_B) = & \\frac{g_i}{\\sqrt{2\\pi e_i^2}}\\exp\\left[\\frac{-\\left(\\hat{y}(x_i~|~\\bar{\\theta}) - y_i\\right)^2}{2e_i^2}\\right] \\\\\n",
      "&+ \\frac{1 - g_i}{\\sqrt{2\\pi \\sigma_B^2}}\\exp\\left[\\frac{-\\left(\\hat{y}(x_i~|~\\bar{\\theta}) - y_i\\right)^2}{2\\sigma_B^2}\\right]\n",
      "\\end{array}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# theta will be an array of length 2 + N, where N is the number of points\n",
      "# theta[0] is the intercept, theta[1] is the slope and theta[2 + i] is the weight g_i\n",
      "def log_prior(theta):\n",
      "    #g_i needs to be between 0 and 1\n",
      "    if (all(theta[2:] > 0) and all(theta[2:] < 1)):\n",
      "        return 0\n",
      "    else:\n",
      "        return -np.inf #log(0) = -inf\n",
      "\n",
      "def log_likelihood(theta, x, y, e, sigma_B):\n",
      "    dy = y - theta[0] - theta[1] * x\n",
      "    g = np.clip(theta[2:], 0, 1)  # g<0 or g>1 leads to NaNs in logarithm\n",
      "    logL1 = np.log(g) - 0.5 * np.log(2 * np.pi * e ** 2) - 0.5 * (dy / e) ** 2\n",
      "    logL2 = np.log(1 - g) - 0.5 * np.log(2 * np.pi * sigma_B ** 2) - 0.5 * (dy / sigma_B) ** 2\n",
      "    return np.sum(np.logaddexp(logL1, logL2))\n",
      "\n",
      "def log_posterior(theta, x, y, e, sigma_B):\n",
      "    lp = log_prior(theta)\n",
      "    \n",
      "    if not np.isfinite(lp):\n",
      "        return -np.inf\n",
      "\n",
      "    return lp + log_likelihood(theta, x, y, e, sigma_B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndim = 2 + len(x) \n",
      "nwalkers = 200   \n",
      "nburn = 10000   \n",
      "nsteps = 16000\n",
      "print 'Number of free parameters in our model =', ndim\n",
      "print 'Number of data points =', x.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Set up starting positions for the walkers'\n",
      "np.random.seed(0)\n",
      "starting_guesses = np.zeros((nwalkers, ndim))\n",
      "starting_guesses[:, :2] = np.random.normal(thetaA, 0.1, size=(nwalkers, 2))\n",
      "starting_guesses[:, 2:] = np.random.uniform(.4, 0.6, size=(nwalkers, ndim - 2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Setup the sampler'\n",
      "pool = Pool(cores) #A hack Dan gave me to not have ghost processes running as with threads keyword\n",
      "#sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, e, 50], pool=pool)\n",
      "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, e, 50])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Running MCMC...\"\n",
      "pos, prob, state = sampler.run_mcmc(starting_guesses, nsteps)\n",
      "print \"Mean acceptance fraction:\", np.mean(sampler.acceptance_fraction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Pull out samples from the chain\n",
      "sample = sampler.chain[:, nburn:, :].reshape(-1, ndim)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have samples, we can exploit a useful property of the Markov Chains. Because their distribution models the posterior, we can integrate out (i.e. marginalise) over nuisance parameters simply by ignoring them.\n",
      "\n",
      "We can look at the (marginalized) distribution of slopes and intercepts by examining the first two columns of the sample:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(sample[:, 0], sample[:, 1], 'sk', alpha=0.1, markersize=0.1)\n",
      "plt.xlabel('intercept')\n",
      "plt.ylabel('slope')\n",
      "plt.xlim(24, 40)\n",
      "plt.ylim(0.3, 0.7)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also look at the one and two dimensional projections of the posterior probability distributions of model parameters. The Figure below shows the marginalized distribution for each parameter independently in the histograms along the diagonal and then the marginalized two dimensional distributions in the other panels. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#note: this is pretty slow, you may wish to skip...\n",
      "triangle.corner(sample, labels=['slope', 'intercept']+['g_%i' % tmp for tmp in range(len(x))]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Find the mean of the samples and outliers with g < 0.5'\n",
      "theta3 = np.mean(sample[:, :2], 0)\n",
      "g = np.mean(sample[:, 2:], 0)\n",
      "outliers = (g < 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Finally we have a robust fit, and we can make probabilistic statements about outliers!'\n",
      "indices = np.random.choice(np.arange(len(sample)), 500)\n",
      "\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.errorbar(x, y, e, fmt='sk', ecolor='gray', label='data', ms=5)\n",
      "for i in indices:\n",
      "    plt.plot(xfit, sample[i, 0] + sample[i, 1]*xfit, '-k', alpha=0.01)\n",
      "\n",
      "plt.plot(xfit, theta[1] + theta[0] * xfit, 'b--', lw=2, label='fit N')\n",
      "plt.plot(xfit, thetaA[0] + thetaA[1] * xfit, 'r--', lw=1, label='fit L1')\n",
      "plt.plot(xfit, theta2[0] + theta2[1] * xfit, 'y--', lw=1, label='fit H')\n",
      "plt.plot(xfit, theta3[0] + theta3[1] * xfit, 'g-', lw=2, label='Proper Fit')\n",
      "plt.plot(x[outliers], y[outliers], 'ro', ms=20, mfc='none', mec='red')\n",
      "plt.title('Maximum Likelihood fit: Bayesian Marginalization')\n",
      "plt.legend(numpoints=1, shadow=True, fancybox=True, loc='lower right')\n",
      "plt.xlim(-1, 100)\n",
      "plt.ylim(20, 85)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Random Notes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Priors\n",
      "\n",
      "The choice of the probability functions for the priors is subjective. Even when we choose the most uninformative priors the posterior estimate is still dependent on this choice. Care must therefore be exercised when choosing the priors.\n",
      "\n",
      "One can choose for example:\n",
      "\n",
      "- A *Flat prior*, i.e. $p(\\theta) \\propto 1$ over a wide range.\n",
      "- A *Noninformative prior*, based on e.g. dimensionality, symmetry, or entropy arguments.\n",
      "- An *Empirical prior*, based on previous unrelated data which constrains the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Marginalization\n",
      "\n",
      "The posterior has some nice properties. For example, it is possible to integrate over some of the parameters in the posterior to find *marginalized* posteriors. This lets us ignore some parts of the fit, usually known as *nuisance parameters*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Units of Probability Density\n",
      "\n",
      "The units of a probability density are the inverse of the units of the quantity in question."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}